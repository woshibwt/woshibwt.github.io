<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>敏捷生活宣言</title>
    <url>/2020/05/22/%E6%95%8F%E6%8D%B7%E7%94%9F%E6%B4%BB%E5%AE%A3%E8%A8%80/</url>
    <content><![CDATA[<h1 id="Mannifesto-for-Agile-Living-Style"><a href="#Mannifesto-for-Agile-Living-Style" class="headerlink" title="Mannifesto for Agile Living Style"></a>Mannifesto for Agile Living Style</h1><h2 id="——每日三省吾身"><a href="#——每日三省吾身" class="headerlink" title="——每日三省吾身"></a>——每日三省吾身</h2><p>敏捷生活精神欢迎变化、提倡定期反思、追求可持续；主张面对面沟通、崇尚简单，有助于长期保持乐观高效的生活状态。</p>
<p>我们最重要的目标，是通过持续不断地尽早调整行为习惯使自己对生活满意。</p>
<p>Our highest priority is to satisfy ourselves through early and continuous change of living habits.</p>
<p>欣然面对生活中或好或坏的变化，即使在低谷期也一样。</p>
<p>Welcome changes in life, even when we are under bad mood. No matter whether changes are “good” or not.</p>
<p>经常性地反思如何能够优化生活，并依此调整自身的行为习惯。相隔几星期或一两个月，倾向于采取较短的周期。</p>
<p>Reflect on how to better life frequently and adjust habits accordingly from a couple of weeks to several months , with a preference to a shorter timescale.</p>
<p>激发自己的斗志，给自己创造需要的环境，并信任自己，从而达成目标。</p>
<p>Make ourselves motivated, give ourselves environment we need, and trust ourselves to achieve goals.</p>
<p>不论何时，面对面的交谈是最富有效果和效率的信息传递方式。</p>
<p>The most efficient and effective method of of conveying information is face-to-face conversation.</p>
<p>可见的实物是进度的首要度量标准。</p>
<p>Visible objects are the primary measure of progress.</p>
<p>敏捷生活倡导可持续生活。敏捷生活者应该能够长期保持稳定的生活。</p>
<p>Agile living style promotes sustainable living style. An “agile liver” should be able to maintain a constant and stable living style.</p>
<p>不断地关注优秀的个体和技能能够增强敏捷生活的能力。</p>
<p>Continuous attention to excellent individuals and techniques enhances agility.</p>
<p>以简洁为本，它是极力减少不必要工作的艺术。</p>
<p>Simplicity is essential because it’s an art of minimizing unnecessary work.</p>
<p>最好的生活方式来源于自我管理。</p>
<p>The best life style comes from self-organizing individuals.</p>
<p>保持敏捷生活的重点在于使得自己到底每一个决策都是基于敏捷的价值和原则的。</p>
<p>The key idea of living agilely is to make each decision based on the principles and values described above.</p>
]]></content>
      <categories>
        <category>生活方式</category>
      </categories>
      <tags>
        <tag>自我提升</tag>
        <tag>敏捷生活</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224N笔记-七-Vanishing Gradient and Fancy RNNs</title>
    <url>/2020/06/14/CS224N%E7%AC%94%E8%AE%B0-%E4%B8%83-Vanishing-Gradient-and-Fancy-RNNs/</url>
    <content><![CDATA[<p>&emsp;&emsp;这节课助教主要介绍了RNN的问题（其实不止在RNN上出现）并且介绍了几种更复杂的RNN变体</p>
<h2 id="Vanishing-gradient-intuition"><a href="#Vanishing-gradient-intuition" class="headerlink" title="Vanishing gradient intuition"></a>Vanishing gradient intuition</h2><p>&emsp;&emsp;RNN理论上可与捕捉较早的历史信息，但是由于梯度消失的原因会导致远程信息无法被有效的被捕捉到。<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/vanishing_gradient_intuition.PNG" alt><br>&emsp;&emsp;当图上这些梯度很小时，反向传播得越深入，梯度信号就会越来越小。<br>&emsp;&emsp;上节课提到，t 时间步下的hidden state表达式为：</p>
<script type="math/tex; mode=display">\boldsymbol{h}^{(t)}=\sigma\left(\boldsymbol{W}_{h} \boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{1}\right)</script><p>&emsp;&emsp;由链式法则可得：</p>
<script type="math/tex; mode=display">\frac{\partial \boldsymbol{h}^{(t)}}{\partial \boldsymbol{h}^{(t-1)}} = diag\left(\sigma^{'}\left(\boldsymbol{W}_{h} \boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{1}\right)\boldsymbol{W}_{h}        \right)</script><p>&emsp;&emsp;考虑一下第$i$步上的损失梯度$J^{(i)}(\theta)$,以及第$j$步上的隐藏状态$h^{(j)}$<img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/chainrule.png" alt><br>&emsp;&emsp;由此可见，如果权重矩阵$W_h$很小，那么这一项会随着$i$和$j$的距离越来越大而变得越来越小。  </p>
<h3 id="why-is-vanishing-gradient-a-problem"><a href="#why-is-vanishing-gradient-a-problem" class="headerlink" title="why is vanishing gradient a problem?"></a>why is vanishing gradient a problem?</h3><p>&emsp;&emsp;<img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/why_vanishing_gradient.png" alt><br>&emsp;&emsp;&emsp;&emsp;来自远处的梯度信号会丢失，因为它比邻近的梯度信号小很多，模型权重只会根据近期效应而不是长期效应进行更新。</p>
<h3 id="Effect-of-vanishing-gradient-on-RNN-LM"><a href="#Effect-of-vanishing-gradient-on-RNN-LM" class="headerlink" title="Effect of vanishing gradient on RNN-LM"></a>Effect of vanishing gradient on RNN-LM</h3><p>例如以下这个语言任务：<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/language_task.png" alt><br>&emsp;&emsp;为了从这个例子中学习，RNN-LM需要对第七步的“tickets”和最后的目标“tickets”之间的依赖关系建模，但是如果梯度很小，模型就不能学习这种依赖关系。<br>&emsp;&emsp;再如下面这个例子：</p>
<p><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/vanishing_problem.PNG" alt><br>&emsp;&emsp;由于梯度的消失，RNN-LMs更善于从顺序近因（sequential recency）学习而不是语法近因（sybtactic recency）。</p>
<h3 id="why-is-exploding-gradient-a-problem"><a href="#why-is-exploding-gradient-a-problem" class="headerlink" title="why is exploding gradient a problem?"></a>why is exploding gradient a problem?</h3><p>&emsp;&emsp;如果梯度过大，则SGD更新步骤过大：<img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/exploding_gradient.png" alt><br>&emsp;&emsp;这可能导致错误的更新：更新得太多，会导致错误的参数配置。在最坏情况下，会导致网络中的<strong>INF</strong>或<strong>NAN</strong>。  </p>
<h3 id="Gradient-clipping-solution-for-exploding-gradient"><a href="#Gradient-clipping-solution-for-exploding-gradient" class="headerlink" title="Gradient clipping: solution for exploding gradient"></a>Gradient clipping: solution for exploding gradient</h3><p>&emsp;&emsp;<strong>梯度裁剪</strong>：如果梯度的范数大于某个阈值，在应用SGD更新前将其缩小：<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/gradient_clipping.png" alt><br>&emsp;&emsp;<strong>思路</strong>：朝着同样的方向迈出一步，但要小一些。<img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/gradient_clipping_pic.png" alt>  </p>
<h3 id="How-to-fix-vanishing-gradient-problem"><a href="#How-to-fix-vanishing-gradient-problem" class="headerlink" title="How to fix vanishing gradient problem?"></a>How to fix vanishing gradient problem?</h3><p>&emsp;&emsp;梯度消失的主要原因是RNN很难学习在多个时间步长下保存信息，在普通的RNN中，隐藏状态被不断地重写：</p>
<script type="math/tex; mode=display">\boldsymbol{h}^{(t)}=\sigma\left(\boldsymbol{W}_{h} \boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x} \boldsymbol{x}^{(t)}+\boldsymbol{b}\right)</script><h3 id="Long-Short-Term-Memory-LSTM"><a href="#Long-Short-Term-Memory-LSTM" class="headerlink" title="Long Short-Term Memory (LSTM)"></a>Long Short-Term Memory (LSTM)</h3><p>&emsp;&emsp;长短期记忆网络是循环神经网络的一个变体，可以有效解决<strong>简单循环神经网络</strong>的梯度爆炸或消失问题。<br>&emsp;&emsp;在第t步，该网络有一个隐藏状态$\boldsymbol{h^{(t)}}$和一个单元状态$\boldsymbol{c^{(t)}}$,它们  </p>
<ul>
<li>都是长度为n的向量</li>
<li>在每个时间步长上，门的每个元素可以打开（1）、关闭（0）或介于两者之间</li>
<li>门是动态的：它们的值是基于上下文计算的<br>&emsp;&emsp;我们有一个输入序列$\boldsymbol{x^{(t)}}$,我们将计算一个隐藏状态$\boldsymbol{h^{(t)}}$和单元状态$\boldsymbol{c^{(t)}}$的序列。在时间步t时，<img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/LSTM.png" alt></li>
<li>遗忘门$\boldsymbol{f^{(t)}}$控制上一个时刻的内部状态$\boldsymbol{c^{(t-1)}}$需要遗忘多少信息。</li>
<li>输入门$\boldsymbol{i^{(t)}}$控制当前时刻的候选状态$\boldsymbol{\widetilde{c}^{(t)}}$有多少信息需要保存。</li>
<li>输出门$\boldsymbol{o^{(t)}}$控制当前时刻的内部状态$\boldsymbol{c^{(t)}}$有多少信息需要输出给外部状态$\boldsymbol{h^{(t)}}$</li>
<li>$\boldsymbol{\widetilde{c}^{(t)}}$是通过非线性函数的得到的<strong>候选状态</strong></li>
<li>新的<strong>内部状态</strong>$\boldsymbol{c^{(t)}}$专门进行线性的循环信息传递 </li>
</ul>
<p>&emsp;&emsp;下面这张图可以更方便来理解LSTM方程<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/LSTM_2.png" alt></p>
<h3 id="How-does-LSTM-solve-vanishing-gradients"><a href="#How-does-LSTM-solve-vanishing-gradients" class="headerlink" title="How does LSTM solve vanishing gradients?"></a>How does LSTM solve vanishing gradients?</h3><p>&emsp;&emsp;RNN的LSTM架构更容易保存许多时间步上的信息（如果遗忘门设置为记得每一时间步上的所有信息，那么单元中的信息被无限地保存）。LSTM并不保证没有消失、爆炸梯度，但是提供了一种更容易的方法来学习远程依赖关系。</p>
<h3 id="Gated-Recurrent-Units-GRU"><a href="#Gated-Recurrent-Units-GRU" class="headerlink" title="Gated Recurrent Units (GRU)"></a>Gated Recurrent Units (GRU)</h3><p>&emsp;&emsp;<strong>门控循环单元</strong>是一种比LSTM网络更加简单的循环神经网络,在每个时间步t上，我们都有输入$x^{(t)}$和隐藏状态$h^{(t)}$而没有单元状态。<br>&emsp;&emsp;GRU引入了一个更新门来控制当前状态需要从历史状态中保留多少信息（不经过非线性变换），同时需要从候选状态中接受多少新信息。</p>
<script type="math/tex; mode=display">\boldsymbol{h^{(t)}} = (1-\boldsymbol{u^{(t)}})\circ \boldsymbol{h^{(t-1)}}+\boldsymbol{u^{(t)}}\circ \boldsymbol{\widetilde{h}^{(t)}}</script><p>&emsp;&emsp;其中$\boldsymbol{u^{(t)}}$为更新门，</p>
<script type="math/tex; mode=display">\boldsymbol{u^{(t)}}=\sigma\left(\boldsymbol{W_uh^{(t-1)}+\boldsymbol{U_ux^{(t)}}+\boldsymbol{b_u}}        \right)</script><p>&emsp;&emsp;在LSTM网络中，输入门和遗忘门是互补关系，具有一定的冗余性。GRU网络直接使用一个门来控制输入和遗忘之间的平衡。</p>
<ul>
<li>当$\boldsymbol{u^{(t)}}=1$时，当前状态$\boldsymbol{h^{(t)}}$和前一时刻的状态$\boldsymbol{h^{(t-1)}}$之间为非线性函数关系</li>
<li>当$\boldsymbol{u^{(t)}}=0$时，$\boldsymbol{h^{(t)}}$与$\boldsymbol{h^{(t-1)}}$之间为线性函数关系。  </li>
</ul>
<p>&emsp;&emsp;在GRU网络中，$\boldsymbol{\widetilde{h}^{(t)}}$表示当前时刻的候选状态</p>
<script type="math/tex; mode=display">\boldsymbol{\widetilde{h}^{(t)}} =\tanh \boldsymbol{\left(W_h(r^{(t)}\circ h^{(t-1)})+U_hx^{(t)}+b_h         \right)}</script><p>&emsp;&emsp;$\boldsymbol{r^{(t)}}$为重置门，用来控制候选状态$\boldsymbol{\widetilde{h}^{(t)}}$的计算是否依赖上一时刻的状态$\boldsymbol{h^{(t-1)}}$</p>
<script type="math/tex; mode=display">\boldsymbol{r^{(t)}} =\sigma\left(\boldsymbol{W_rh^{(t-1)}+\boldsymbol{U_rx^{(t)}}+\boldsymbol{b_r}}        \right)</script><ul>
<li>当$\boldsymbol{r^{(t)}}=0$时，候选状态$\boldsymbol{\widetilde{h}^{(t)}}=\tanh\boldsymbol{(U_hx^{(t)}+b)}$只和当前输入$\boldsymbol{x^{(t)}}$有关，和历史状态无关</li>
<li>当$\boldsymbol{r^{(t)}}=1$时，候选状态$\boldsymbol{\widetilde{h}^{(t)}}=\tanh\boldsymbol{(W_hh^{(t-1)}+U_hx^{(t)}+b)}$和当前输入$\boldsymbol{x^{(t)}}$以及历史状态$\boldsymbol{h^{(t-1)}}$相关，和<strong>简单循环网络</strong>一致。</li>
</ul>
<p>&emsp;&emsp;综上，GRU网络的状态更新方式为</p>
<script type="math/tex; mode=display">\boldsymbol{h^{(t)}} = (1-\boldsymbol{u^{(t)}})\circ \boldsymbol{h^{(t-1)}}+\boldsymbol{u^{(t)}}\circ \boldsymbol{\widetilde{h}^{(t)}}</script><p>&emsp;&emsp;可以看出，</p>
<ul>
<li>当$\boldsymbol{u^{(t)}}=1，\boldsymbol{r}=1$时，GRU网络退化为<strong>简单循环网络</strong></li>
<li>若$\boldsymbol{u^{(t)}}=1，\boldsymbol{r}=0$时，当前状态$\boldsymbol{h^{(t)}}$只和当前输入$\boldsymbol{x^{(t)}}$相关，和历史状态$\boldsymbol{h^{(t-1)}}$无关。</li>
<li>当$\boldsymbol{u^{(t)}}=0$时，当前状态$\boldsymbol{h^{(t)}}$等于上一时刻状态$\boldsymbol{h^{(t-1)}}$，和当前输入$\boldsymbol{x^{(t)}}$无关。</li>
</ul>
<p><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/GRU.png" alt></p>
]]></content>
      <categories>
        <category>CS224N</category>
      </categories>
      <tags>
        <tag>nlp 机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224N笔记-六-Language Modeling And RNN</title>
    <url>/2020/06/12/CS224N%E7%AC%94%E8%AE%B0-%E5%85%AD-Language-Modeling-And-RNN/</url>
    <content><![CDATA[<p>本次课程助教小姐姐主要介绍的是<strong>Language Modeling</strong>与<strong>Recurrent Neural Network(RNN)</strong></p>
<h2 id="Language-Modeling"><a href="#Language-Modeling" class="headerlink" title="Language Modeling"></a>Language Modeling</h2><p>&emsp;&emsp;语言模型研究的是根据已知的序列来推测下一个单词的问题，也就是假设给定一个单词序列$x^{(1)},x^{(2)},…,x^{(t)}$，计算下一个单词$x^{(t+1)}$的概率分布</p>
<script type="math/tex; mode=display">P(x^{(t+1)}|x^{(t)},...,x^{(1)})</script><p>&emsp;&emsp;根据条件概率的链式法则，我们也可以将其看作一系列词出现的概率问题：</p>
<script type="math/tex; mode=display">P(x^{(t)},...,x^{(1)}) = \prod_{t=1}^{T} P\left(x^{(t)}|x^{(t-1)},...,x^{(1)}\right)</script><h2 id="n-gram-Language-Models"><a href="#n-gram-Language-Models" class="headerlink" title="n-gram Language Models"></a>n-gram Language Models</h2><p>&emsp;&emsp;n-gram模型的定义就是连续的n个单词组成的块，例如对于“the students opened their __”这句话：</p>
<ul>
<li>unigrams:”the”,”student”,”opened”,”their”</li>
<li>biagrams:”the students”,”students opened”,”opend their”</li>
<li>trigrams:”the students opened”,”students opened their”</li>
<li>4-grams:”the students opened their”<br>&emsp;&emsp;该模型的核心思想是n-gram的概率应正比于器出现的频率，所以可以用此模型预测下一个单词。<br>&emsp;&emsp;假设$P(x^{(t+1)})$仅依赖它之前的n-1个单词，即<script type="math/tex; mode=display">P(x^{(t+1)}|x^{(t)},...,x^{(1)}) = P(x^{(t+1)}|x^{(t)},...,x^{(t-n+2)}) = \\
\frac{ P(x^{(t+1)},x^{(t)},...,x^{(t-n+2)})}{P(x^{(t)},...,x^{(t-n+2)})}\approx \frac{ count(x^{(t+1)},x^{(t)},...,x^{(t-n+2)})}{count (x^{(t)},...,x^{(t-n+2)})}</script>&emsp;&emsp;其中count是通过处理大量文本相对应的n-gram出现次数得到的。</li>
</ul>
<p>&emsp;&emsp;假设在学习一个<strong>4-gram</strong>的语言模型：<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/4-gram-model.png" alt><br>&emsp;&emsp;在语料库中：</p>
<ul>
<li>“students opened their”出现了1000次</li>
<li>“students opened their books”出现了400次<script type="math/tex; mode=display">P(books|students\ opened\ their) = 0.4</script></li>
<li>“students opened their exams”出现了100次<script type="math/tex; mode=display">P(exams|students\ opened\ their) = 0.1</script></li>
<li>由于上下文出现了“proctor”,所以在这里exams的概率应当比book更大<h3 id="Sparsity-Problems-with-n-gram-Language-Models"><a href="#Sparsity-Problems-with-n-gram-Language-Models" class="headerlink" title="Sparsity Problems with n-gram Language Models"></a>Sparsity Problems with n-gram Language Models</h3><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/SparsityProblems.png" alt> <h3 id="Storage-Problems-with-n-gram-Language-Models"><a href="#Storage-Problems-with-n-gram-Language-Models" class="headerlink" title="Storage Problems with n-gram Language Models"></a>Storage Problems with n-gram Language Models</h3><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/storage_problems.png" alt></li>
</ul>
<h3 id="How-to-build-a-neural-Language-Model"><a href="#How-to-build-a-neural-Language-Model" class="headerlink" title="How to build a neural Language Model?"></a>How to build a neural Language Model?</h3><p>&emsp;&emsp;可以使用在第三讲中被用于NER问题的<strong>window-based neural model</strong><br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/Ner.png" alt></p>
<p>&emsp;&emsp;对于下列句子：<img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/sentence.png" alt><br>&emsp;&emsp;使用上述模型网络结构<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/network.png" alt></p>
<h4 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h4><ul>
<li>固定窗口大小</li>
<li>扩大窗口就要扩大权重矩阵$W$</li>
<li>窗口再大也不够用</li>
<li>$x^{(1)}和x^{(2)}$乘以不同的权重，输入的处理不整齐。  </li>
</ul>
<p>需要一个可以处理任何长度的输入的神经网络。</p>
<h2 id="Recurrent-Neural-Networks-RNN"><a href="#Recurrent-Neural-Networks-RNN" class="headerlink" title="Recurrent Neural Networks (RNN)"></a>Recurrent Neural Networks (RNN)</h2><p>&emsp;&emsp;RNN（Recurrent Neural Network）结构通过不断应用<strong>同一个</strong>矩阵$W$来实现参数的有效共享(个人推荐<a href="https://zhuanlan.zhihu.com/p/30844905" target="_blank" rel="noopener">这一篇</a>扫盲)<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/RNN1.png" alt></p>
<p><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/RNN2.png" alt></p>
<p>RNN的优点</p>
<ul>
<li>可以处理<strong>任意长度</strong>的输入</li>
<li>步骤t的计算理论上可以使用许多<strong>步骤前</strong>的信息</li>
<li>模型大小<strong>不会</strong>随着输入的增加而增加</li>
<li>在每个time step上都应用相同的权重，因此在处理输入时具有<strong>一致性</strong>。  </li>
</ul>
<p>RNN的缺点</p>
<ul>
<li>递归计算速度慢</li>
<li>在实践中，很难从<strong>许多步骤前</strong>获得信息</li>
<li>更多后续课程中会提到</li>
</ul>
<p>&emsp;&emsp;RNN的训练过程依赖于大量文本，在每个时刻t会计算模型预测的输出$y^{(t)}$与真实值$\hat{y}^{(t)}$(即$x^{(t+1)}$的one-hot向量)的交叉熵：</p>
<script type="math/tex; mode=display">\begin{aligned}
J^{(t)}(\theta) = CE(y^{(t)},\hat{y}^{(t)}) &= -\sum_{w\in V} y^{(t)}_w \log \hat{y}^{(t)}_w \\ &= -\log\hat{y}^{(t)}_{x_{t+1}}
\end{aligned}</script><p>&emsp;&emsp;将其平均，得到整个训练集的overall loss</p>
<script type="math/tex; mode=display">J(\theta) = \frac{1}{T}\sum^T_{t=1}J^{(t)}(\theta) = \frac{1}{T}-\log\hat{y}^{(t)}_{x_{t+1}}</script><p><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/loss.png" alt></p>
<script type="math/tex; mode=display">\begin{aligned}
J^{(1)}(\theta)+J^{(2)}(\theta)+J^{(3)}(\theta)+... &= J(\theta) \\ &=  \frac{1}{T}\sum^T_{t=1}J^{(t)}(\theta)
\end{aligned}</script><p>&emsp;&emsp;然而，计算<strong>整个</strong>语料库$x^{(1)},…,x^{(T)}$的损失和梯度成本太过昂贵了，在实践中，我们将$x^{(1)},…,x^{(T)}$看作一个<strong>句子</strong>或<strong>文档</strong>，应用<strong>随机梯度下降算法</strong>计算小块数据的损失和梯度，并进行更新</p>
<h3 id="Backpropagation-for-RNNs"><a href="#Backpropagation-for-RNNs" class="headerlink" title="Backpropagation for RNNs"></a>Backpropagation for RNNs</h3><p><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/BackPropagation.png" alt><br>&emsp;&emsp;重复权重的梯度是每次其出现时的梯度总和</p>
<script type="math/tex; mode=display">\frac{\partial J^{(t)}}{\partial W_h}=  \sum_{i=1}^t \frac{\partial J^{(t)}}{\partial W_h} \bigg|_{(i)}</script><p><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/caluclate_gradient.png" alt></p>
<h3 id="Evaluating-Language-Models"><a href="#Evaluating-Language-Models" class="headerlink" title="Evaluating Language Models"></a>Evaluating Language Models</h3><p>&emsp;&emsp;标准语言模型评估指标是perplexity 困惑度<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/perplexity-困惑度.png" alt><br>&emsp;&emsp;等于交叉熵损失$J(\theta)$的指数</p>
<script type="math/tex; mode=display">\begin{aligned}
=\prod_{t=1}^T \left( {\frac{1}{\hat y ^ {(t)}_{x_{t+1}}}} \right)^{1/T} &= \exp \left( \frac{1}{T}\sum_{t=1}^T-\log \hat y ^ {(t)}_{x_{t+1}} \right) \\ &= \exp(J(\theta))  
\end{aligned}</script><p>&emsp;&emsp;低困惑度是好的</p>
<h3 id="Why-should-we-care-about-Language-Modeling"><a href="#Why-should-we-care-about-Language-Modeling" class="headerlink" title="Why should we care about Language Modeling?"></a>Why should we care about Language Modeling?</h3><p>&emsp;&emsp;语言模型是一项<strong>基准测试</strong>任务，它帮助我们<strong>衡量</strong>我们在理解语言方面的<strong>进展</strong><br>&emsp;&emsp;语言建模是许多NLP任务的<strong>子组件</strong>，尤其是那些涉及<strong>生成文本</strong> 或<strong>估计文本概率</strong>的任务</p>
]]></content>
      <categories>
        <category>CS224N</category>
      </categories>
      <tags>
        <tag>nlp 机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224N笔记（三，四）-Word Window Classification,Nerual Networks,Matrix Calculus</title>
    <url>/2020/06/04/CS224N%E7%AC%94%E8%AE%B0-%EF%BC%88%E4%B8%89%EF%BC%8C%E5%9B%9B%EF%BC%89-Word-Window-Classification-Nerual-Networks-Matrix-Calculus/</url>
    <content><![CDATA[<p>&emsp;&emsp;本次课程主要介绍了神经网络的基础知识并以<strong>命名实体识别Named Entity Recognition(NER)</strong>来介绍DNN在NLP领域内的应用。  </p>
<h2 id="分类的设置与符号"><a href="#分类的设置与符号" class="headerlink" title="分类的设置与符号"></a>分类的设置与符号</h2><p>&emsp;&emsp;通常我们有由样本组成的训练数据集</p>
<script type="math/tex; mode=display">\{x_i,y_i\}^{N}_{i=1}</script><p>&emsp;&emsp;其中$x_i$是输入，例如单词，句子，文档，维度为$d$<br>&emsp;&emsp;而$y_i$是我们尝试预测的标签（C种类别中的其中一种）  </p>
<h3 id="直观分类"><a href="#直观分类" class="headerlink" title="直观分类"></a>直观分类</h3><p><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/lesson34_ClassificationIntuition.PNG" alt></p>
<ul>
<li>固定的二维单词分类</li>
<li>使用softmax/logistic回归</li>
<li>线性决策边界</li>
</ul>
<p><strong>传统的机器学习、统计学方法</strong>：假设$x_i$是固定的，训练softmax/logistic回归的权重$W \in \mathbb{R}^{C\times d}$来决定边界（超平面）<br><strong>方法</strong>：对每个$x$,预测</p>
<script type="math/tex; mode=display">p(y|x) = \frac{\exp(W_y\cdot x)}{\sum_{c=1}^C\exp(W_c \cdot x)}</script><p>&emsp;&emsp;将预测函数分为两个步骤：<br>&emsp;&emsp;1.将$W$的$y^{th}$行和$x$中的对应行相乘得到分数</p>
<script type="math/tex; mode=display">W_y \cdot x = \sum_{i=1}^dW_{yi}x_i = f_y</script><p>&emsp;&emsp;计算所有的$f_c,for \ c =1,…,C$<br>&emsp;&emsp;2.使用softmax函数获得归一化的概率</p>
<script type="math/tex; mode=display">p(y|x) = \frac{\exp(f_y)}{\sum_{c=1}^C\exp(f_c)} = softmax(f_y)</script><h3 id="使用softmax与交叉熵训练"><a href="#使用softmax与交叉熵训练" class="headerlink" title="使用softmax与交叉熵训练"></a>使用softmax与交叉熵训练</h3><p>&emsp;&emsp;对于每个训练样本$(x,y)$,我们的目标是最大化正确类$y$的概率，或者最小化该类的负对数概率：</p>
<script type="math/tex; mode=display">-\log p(y|x) = - \log\left(\frac{\exp(f_y)}{\sum_{c=1}^C\exp(f_c)}\right)</script><h3 id="什么是交叉熵"><a href="#什么是交叉熵" class="headerlink" title="什么是交叉熵"></a>什么是交叉熵</h3><ul>
<li>其概念来源于信息论，用于衡量两个分布之间的差异</li>
<li>令真实分布为$p$</li>
<li>令我们计算的模型概率为$q$</li>
<li>交叉熵即为：<script type="math/tex; mode=display">H(p,q) = -\sum_{c=1}^Cp(c)\log q(c)</script><h3 id="整个数据集上的分类"><a href="#整个数据集上的分类" class="headerlink" title="整个数据集上的分类"></a>整个数据集上的分类</h3>&emsp;&emsp;在整个数据集${\{x_i,y_i\}}_{i=1}^N$上的交叉熵损失函数，是所有样本的交叉熵的均值：<script type="math/tex; mode=display">J(\theta) = \frac{1}{N}\sum_{i=1}^N-log\left(\frac{\exp(f_{yi})}{\sum_{c=1}^C\exp(f_c)}\right)</script><h3 id="传统的机器学习优化"><a href="#传统的机器学习优化" class="headerlink" title="传统的机器学习优化"></a>传统的机器学习优化</h3>&emsp;&emsp;一般机器学习的参数$\theta$通常只由$W$的列组成：</li>
</ul>
<script type="math/tex; mode=display">\theta = \begin{bmatrix}
W_1          \\
.            \\
.            \\
.            \\
W_d

\end{bmatrix} \in W(:)\in \mathbb{R}^{Cd}</script><p>&emsp;&emsp;因此，我们通过以下方式更新决策边界：</p>
<script type="math/tex; mode=display">\nabla_\theta J(\theta) = \begin{bmatrix}
\nabla_{W_1} \\
.            \\
.            \\
.            \\
\nabla_{W_d}

\end{bmatrix} \in \mathbb{R}^{Cd}</script><h3 id="神经网络分类器"><a href="#神经网络分类器" class="headerlink" title="神经网络分类器"></a>神经网络分类器</h3><p><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/lesson34_ClassificationIntuition.PNG" alt></p>
<ul>
<li>单独使用Softmax(或logistic回归)效果并不是很好</li>
<li>softmax只能给出线性决策边界   </li>
</ul>
<p>&emsp;&emsp;<strong>神经网络基础知识在这里省略</strong>😀</p>
<h2 id="命名实体识别（NER）"><a href="#命名实体识别（NER）" class="headerlink" title="命名实体识别（NER）"></a>命名实体识别（NER）</h2><ul>
<li>任务：比如查找和分类文本中的名称<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/lesson34_NERTask.PNG" alt></li>
<li>可能的用途<ul>
<li>跟踪文档中提到的特定实体</li>
<li>对于问答，答案通常是命名实体</li>
<li>许多需要的信息实施上是命名实体之间的关联  </li>
<li>&emsp;&emsp;我们通过在上下文中对单词进行分类，然后将实体提取为单词子序列来预测实体。<h3 id="为什么NER可能相当困难"><a href="#为什么NER可能相当困难" class="headerlink" title="为什么NER可能相当困难"></a>为什么NER可能相当困难</h3></li>
</ul>
</li>
<li>很难计算出实体的边界</li>
<li>很难知道某物是否是一个实体</li>
<li>很难知道新奇实体的类别</li>
<li>实体是模糊的，依赖于上下文<h3 id="Window-Classification"><a href="#Window-Classification" class="headerlink" title="Window Classification"></a>Window Classification</h3>&emsp;&emsp;由于同一个词在不同上下文中可能是不同的命名实体，因此一个思路是通过对该词在某一窗口内附近的词来对其进行分类(类别包括人民，地名，机构名等)。<br>&emsp;&emsp;对于museums in Paris are amazing,我们希望探测到地名Paris。假设窗口大小为2，并且通过word2vec得到窗口内五个单词的词向量，则我们可以将这5个向量连起来得到更大的向量，再对该向量进行分类。<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/lesson34_Paris.PNG" alt><br>&emsp;&emsp;对于具有多个class的分类问题，我们通常用softmax classofier来解决假设用$x$来表示输入的词向量，$y$来表示对应的class，总计有$C$个class,则$x$对应类别为$y$的概率为：<script type="math/tex; mode=display">p(y|x) = \frac{\exp(W_y\cdot x)}{\sum_{c=1}^C\exp(W_c \cdot x)}</script>&emsp;&emsp;损失函数为交叉熵：<script type="math/tex; mode=display">J(\theta) = \frac{1}{N}\sum_{i=1}^N-log\left(\frac{\exp(f_{yi})}{\sum_{c=1}^C\exp(f_c)}\right)</script>&emsp;&emsp;为了处理输入元素间的非线性关系，我们可以利用神经网络，并且输出层是每一个class概率的softmax layer。<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/lesson34_NerualNetworkParis.jpg" alt><br>之后可以使用随机梯度下降算法SGD来更新网络并利用<a href>反向传播算法</a>来有效率的计算梯度。</li>
</ul>
<h3 id="Binary-classification-with-unnormalized-scores"><a href="#Binary-classification-with-unnormalized-scores" class="headerlink" title="Binary classification with unnormalized scores"></a>Binary classification with unnormalized scores</h3><p>&emsp;&emsp;对于之前的例子$X_{window} = [X_{museum} \ \ \ X_{in}\ \ \ X_{paris}\ \ \ X_{are} \ \ \ X_{amazing}]$。假设我们要对中心词是否为一个地点进行分类，与word2vec类似，我们将便利语料库中的所有位置，但这一次只有一些位置能获得高分，例如，窗口的中心有一个实际的NER Location时会获得高分。</p>
<h3 id="Binary-classification-with-unnormalized-scores-1"><a href="#Binary-classification-with-unnormalized-scores-1" class="headerlink" title="Binary classification with unnormalized scores"></a>Binary classification with unnormalized scores</h3><p>&emsp;&emsp;例子：<strong>Not all museums in Paris are amazing</strong><br>&emsp;&emsp;在这里只有以<strong>Paris</strong>为中心的窗口是真正的窗口，所有其他窗口都”毁坏了”，因为它们的中心没有具体的实体位置。</p>
<h3 id="Binary-classification-with-unnormalized-scores-2"><a href="#Binary-classification-with-unnormalized-scores-2" class="headerlink" title="Binary classification with unnormalized scores"></a>Binary classification with unnormalized scores</h3><p>&emsp;&emsp;使用神经激活$a$简单给出一个非标准化的分数</p>
<script type="math/tex; mode=display">score(x) = U^Ta\in\mathbb{R}</script><p>&emsp;&emsp;在这里用一个三层神经网络计算一个窗口的得分</p>
<script type="math/tex; mode=display">s = score("museums in Paris are amazing")</script><script type="math/tex; mode=display">s = U^Tf(Wx+b)</script><script type="math/tex; mode=display">x\in\mathbb{R}^{20\times 1},W\in\mathbb{R}^{8\times20},U\in\mathbb{R}^{8\times1}</script><h3 id="The-max-margin-loss"><a href="#The-max-margin-loss" class="headerlink" title="The max-margin loss"></a>The max-margin loss</h3><ul>
<li>训练目标，让真实窗口的得分更高，让被损坏的窗口得分更低</li>
<li>真实窗口：$s = score(“museums \ in \ Paris \ \ are \ amazing”)$</li>
<li>被损坏的窗口$sc = score(“Not \ all \ museums \ in \ Paris”)$</li>
<li>最小化$J=max(0)$</li>
<li>可以使用SGD,单窗口的目标函数为：$J = max(0,1-s+s_c)$</li>
<li>每个中心有Ner位置的窗口得分比被损坏的窗口高1分</li>
<li>要获得完整的目标函数：为每个真实窗口采样几个损坏的窗口。对所有窗口求和。<h3 id="Example-Jacobian-Elementwise-activation-Function"><a href="#Example-Jacobian-Elementwise-activation-Function" class="headerlink" title="Example Jacobian: Elementwise activation Function"></a>Example Jacobian: Elementwise activation Function</h3><script type="math/tex; mode=display">h = f(z),\frac{\partial h}{\partial z} = ?,\mathbf{h,z}\in\mathbb{R}^n</script></li>
</ul>
]]></content>
      <categories>
        <category>CS224N</category>
      </categories>
      <tags>
        <tag>nlp 机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224N笔记 (二):Glove</title>
    <url>/2020/05/28/cs224N-CS224N%E7%AC%94%E8%AE%B0-%E4%BA%8C-Glove/</url>
    <content><![CDATA[<p>&emsp;&emsp;上一篇笔记<a href="https://woshibwt.com/2020/05/25/CS224N%E7%AC%94%E8%AE%B0-%E4%B8%80-word-vector/#more" target="_blank" rel="noopener">CS224N笔记(一):word vector</a>中教授主要讲课重点是基于local context window的<strong>Word2Vec</strong>模型，它是一种direct prediction模型。那么对于word vector是否还存在其他模型呢，答案当然是有，本次介绍的就是另一类count based global matrix factorization模型以及Manning教授结合这两种模型的优点提出的<strong>Glove</strong>模型。<br>&emsp;</p>
<h2 id="回顾：word2vec的主要思想"><a href="#回顾：word2vec的主要思想" class="headerlink" title="回顾：word2vec的主要思想"></a>回顾：word2vec的主要思想</h2><p><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/lesson1_centerword_into.PNG" alt></p>
<script type="math/tex; mode=display">P(o|c) = \frac{\exp(u_o^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}</script><ul>
<li>遍历整个语料库中的每个单词</li>
<li>使用单词向量更好的预测周围的单词</li>
<li>更新向量以更好的预测</li>
</ul>
<p><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/lesson2_Word2vex_ParametersAndComputations.PNG" alt></p>
<blockquote>
<p>在这张图中，选取的特定向量是v矩阵的第四行与U的每一行向量做点积</p>
<ul>
<li>在这张图以及许多深度学习包中，每一行代表一个单词的词向量。经过点乘操作后的数字通过softmax映射为概率分布，注意该概率分布是对于该中心词而言的上下文中的概率分布，该分布与上下文所在的具体位置无关。</li>
<li>我们希望对上下文中（相当频繁）出现的所有单词给出一个合理的概率统计。</li>
<li>the,and,that,of这类的停用词，有时简单地去掉这一部分可以使词向量效果更好。</li>
</ul>
</blockquote>
<h3 id="梯度下降的优化"><a href="#梯度下降的优化" class="headerlink" title="梯度下降的优化"></a>梯度下降的优化</h3><p><strong>Gradient Descent</strong>： 每次使用全部样本进行更新<br><strong>Stochastic Gradient Decent</strong>:每次只使用单个样本更新<br><strong>Mini-batch</strong>:</p>
<ul>
<li>通过平均值，减少梯度估计的噪音</li>
<li>可在GPU上并行化运算，加快运算速度<h4 id="词向量的随机梯度下降"><a href="#词向量的随机梯度下降" class="headerlink" title="词向量的随机梯度下降"></a>词向量的随机梯度下降</h4><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/lesson2_SGDvector.PNG" alt></li>
</ul>
<p>由于$\nabla_{\theta}J_t(\theta)$将会非常稀疏，因此我们只更新实际出现的向量。<br>解决方案  </p>
<ul>
<li>需要稀疏矩阵更新操作来只更新矩阵U和V中的特定行</li>
<li>需要保留单词向量的散列<br>如果有数百万个单词向量，并且进行分布式计算，那么不用到处发送巨大的更新是很重要的。  <h4 id="更多关于word2vec的细节"><a href="#更多关于word2vec的细节" class="headerlink" title="更多关于word2vec的细节"></a>更多关于word2vec的细节</h4>为什么需要两个向量？</li>
<li>更容易优化，最后可对两个向量取平均值  </li>
</ul>
<p>两个模型变体</p>
<ul>
<li><strong>Skip-grams(SG)</strong><ul>
<li>输入中心词并预测上下文中的单词(正是课堂中展示的这一种)</li>
</ul>
</li>
<li><strong>Continuous Bag of Words(CBOW)</strong><ul>
<li>输入上下文中的单词并预测中心词    </li>
</ul>
</li>
</ul>
<p>提升训练效率<br>&emsp;&emsp;之前选择的softmax训练方法虽然简单但代价很高，考虑使用负采样方法加快训练速率。<br>&emsp;&emsp;<a href>作业二</a>实现了使用<strong>nagative sampling</strong>(负采样方法)的skip-gram模型</p>
<ul>
<li>对一个由true pair(中心词及其上下文窗口中的词)与几个noise pair(中心词与随机词搭配)形成的样本，训练二元逻辑回归。  </li>
</ul>
<p>在论文中的(最大化)目标函数是  </p>
<script type="math/tex; mode=display">J(\theta)=\frac{1}{T}\sum_{t=1}^TJ_t(\theta)</script><script type="math/tex; mode=display">J_t(\theta) = \log\sigma(u_o^Tv_c)+\sum_{i=1}^k\mathbb {E}_{j\sim P(w)}\left[\log\sigma\left(-u_j^Tv_c\right)\right]</script><p>本课及作业中的目标函数是  </p>
<script type="math/tex; mode=display">J_{neg-sample}(o,v_c,U) = -log(\sigma(u_o^Tv_c)) - \sum_{k=1}^K\log(\sigma(-u_k^Tv_c))</script><ul>
<li>我们希望中心词与真实上下文单词的向量点积更大，中心词与随机单词的点击更小</li>
<li>k是负采样的样本数目<script type="math/tex; mode=display">P(w) = U(w)^{3/4}/Z</script>&emsp;&emsp;使用上式作为抽样的分布，$U(w)$是<a href>unigram分布</a>，通过$\frac{3}{4}$次方，相对减少常见单词的频率，增大稀有词的频率。$Z$用于生成概率分布。  <h2 id="SVD模型"><a href="#SVD模型" class="headerlink" title="SVD模型"></a>SVD模型</h2>&emsp;&emsp;暂时放下local context window的模型，现在来讨论count based模型，它的一个经典代表是<strong>SVD</strong>（Single Value Decomposition）模型：我们先扫过一遍所有数据，然后得到共现矩阵(co-occurance matrix),假设矩阵用$X$表示，我们对其使用SVD得到$X$的分解形式$USV^T$ (详情请见<a href="https://zhuanlan.zhihu.com/p/29846048" target="_blank" rel="noopener">奇异值分解</a>)<br>&emsp;&emsp;如何产生矩阵$X$通常有两种选择：</li>
<li><strong>word-document co-occurrence matrix</strong>:其基本假设是在同一篇文章中出现的单词更有可能相互关联。假设单词$i$出现在文章$j$中，则矩阵元素$X_{ij}$加一，当我们处理完所有的文章后，就得到了矩阵$X$,其大小为$\lvert V \rvert \times M$,其中$\lvert V \rvert$为词汇量，而$M$为文章数</li>
<li><p><strong>window-based word-word co-occurrence matrix</strong>:该矩阵是利用某个定长窗口中单词与单词同时出现的个数来产生，下面用窗口长度为1来举例(常见的是5-10)，假设我们的数据包含以下几个句子：</p>
<p><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/lesson1_ExampleCorpus.PNG" alt><br>则根据上述原理，我们可以获得如下的word-word co-occurrence matrix:</p>
<p><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/lesson2_word-wordMatrix.PNG" alt></p>
<p>&emsp;&emsp;使用共现次数可以衡量单词的相似性，但是随着词汇量的增加，矩阵的大小也会增加，需要很多空间来存储这一高维矩阵，后续的分类模型也会由于矩阵的稀疏性而存在稀疏性问题，因此我们需要对这一矩阵进行降维，获得低维（25-1000）的稠密向量。</p>
</li>
<li><strong>Method 1: Dimensionality Reduction on X (HW1)</strong>:可以使用上文提到的SVD方法将共现矩阵分解。为了减少维数同时尽量保存有效信息，可保留对角矩阵的最大k个值，并将矩阵$U$,$V$相应的行列保留，对于大型矩阵而言，计算代价昂贵。</li>
<li><p><strong>Hacks to X (several used in Rohde et al. 2005)</strong><br>按比例调整计数会很有效</p>
<ul>
<li>对高频词进行缩放<ul>
<li>使用$log$进行缩放</li>
<li>$min(X,t),t\approx100$</li>
<li>直接全部忽视</li>
</ul>
</li>
<li>在基于窗口的计数中，提高更加接近的单词的计数</li>
<li>使用Person相关系数 </li>
</ul>
<p>实验结果如下图：<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/lesson2_InterestingSemanticPatterns.PNG" alt><br>$swim\to swimmer , drive \to driver$<br>&emsp;&emsp;在向量中出现了有趣的句法模式，语义向量基本上是动词和动词实施者的方向。</p>
</li>
</ul>
<p>count based:使用整个矩阵的全局统计数据来直接估计</p>
<ul>
<li>优点<ul>
<li>训练快速</li>
<li>统计数据高效利用</li>
</ul>
</li>
<li>缺点<ul>
<li>主要用于捕捉单词相似性</li>
<li>对大量数据给予比例失调的重视  </li>
</ul>
</li>
</ul>
<p>direct prediction：定义概率分布并试图预测单词</p>
<ul>
<li>优点<ul>
<li>提高其他任务的性能</li>
<li>能捕获包括单词相似性等复杂模式</li>
</ul>
</li>
<li>缺点<ul>
<li>量度与语料库相关</li>
<li>统计数据的低效使用</li>
</ul>
</li>
</ul>
<h2 id="Glove算法"><a href="#Glove算法" class="headerlink" title="Glove算法"></a>Glove算法</h2><p> &emsp;&emsp;Manning教授团队希望采取一种方法可与结合上述两种方法的优势，并将这种算法命名为<strong>GloVe</strong>(Global vectors的缩写)，该算法可以有效地利用全局地统计信息。<br> &emsp;&emsp;如何使用word-word co-occurance count来学习词语背后的含义呢？<br> &emsp;&emsp;首先定义一些符号：对于矩阵$X$,$X_{ij}$代表了单词$j$出现在单词$i$的上下文中的单词次数，则$X_i=\sum_{k}X_{ik}$,即代表了所有出现在单词$i$的上下文中的单词个数，我们用$P_{ij}=P(j|i)=X_{ij}/X_i$来代表单词$j$出现在单词$i$上下文中的概率。</p>
<blockquote>
<p>算法关键思想：共现概率的<strong>比值</strong>(而不是概率单一大小)可以对meaning component进行编码  </p>
</blockquote>
<p> &emsp;&emsp;来参考课上给出的例子：<br> <img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/lesson2_RatiosOfCo-occurence.PNG" alt><br>&emsp;&emsp;例如我们想区分热力学上两种不同状态ice与steam,它们之间的关系可通过与不同单词$x$的co-occurrence probablity的<strong>比值</strong>来描述，例如对于solid,虽然$P(solid|ice)$与$P(solid|steam)$本身很小，不能透露有效的信息，但是它们的<strong>比值</strong>$\frac{P(solid|ice)}{P(solid|steam)}$较大，因为solid更多用来描述ice的状态而不是steam的状态，因此在ice的上下文中出现几率较大，对于gas情况则恰恰相反。而对于water这种描述ice与steam皆可或者fashion这种与两者都没什么联系的单词，比值接近于1.所以实际上co-occurence probablity的相对比值更有意义。<br>&emsp;&emsp;基于这些观察，课堂上直接给出了GloVe的损失函数形式：</p>
<script type="math/tex; mode=display">w_j \cdot w_j = \log P(i|j)</script><script type="math/tex; mode=display">J = \sum_{i,j=1}^V {f(X_{ij})(w_i^T\widetilde{w}_j+b_i+\widetilde{b}_j-\log X_{ij})}^2</script><p>&emsp;&emsp;基于对以上概率比值的观察，我们假设模型的函数有如下形式：</p>
<script type="math/tex; mode=display">F(w_i,w_j,\widetilde{w}_k) = \frac{P_{ik}}{P_{jk}}</script><p>&emsp;&emsp;其中$\widetilde{w}$代表了上下文向量，如上文的solid,gas,water,fashion。$w_i,w_j$则是我们要比较的两个词汇，如上文的ice,steam。</p>
<blockquote>
<p>&emsp;&emsp;F可选的形式很多，由于希望$F$能在单词向量空间内表示概率比值，有意向量空间是线性空间，一个自然假设是$F$是关于向量$w_i,w_j$的差的形式：</p>
<script type="math/tex; mode=display">F((w_i-w_j),\widetilde{w}_k) = \frac{P_{ik}}{P_{jk}}</script><p>等式右边为标量形式，为了将左边矢量也转化为标量形式，使用矢量的点乘:</p>
<script type="math/tex; mode=display">F((w_i-w_j)^T\widetilde{w}_k) = \frac{P_{ik}}{P_{jk}}</script><p>&emsp;&emsp;在此，作者又进行了对称性分析，即对于word-word co-occurrence,将一个vector划分为center word还是context word是不重要的，即在交换$w\Leftrightarrow \widetilde{w}$与$X \Leftrightarrow X^T$的时候仍然成立，如何满足这种对称性呢？<br>&emsp;&emsp;接下来分成两部来进行<br>&emsp;&emsp;首先要求满足$F((w_i-w_j)^T \widetilde{w}_k)=\frac{F(w_i^T\widetilde{w}_k)}{F(w_j^T\widetilde{w}_k)}$,显然该方程的解为$F=\exp$，同时也由此找对应项  </p>
</blockquote>
<p>&emsp;$F((w_i-w_j)^T\widetilde{w}_k) = \frac{P_{ik}}{P_{jk}} \\\Rightarrow F(w_i^T\widetilde{w}) = P_{ik} = \frac{X_{ik}}{X_i}\\\Rightarrow w_i^T\widetilde{w}_k = \log(P_{ik})=\log(X_{ik}) - \log(X_i)$  </p>
<blockquote>
<p>&emsp;&emsp;注意其中$\log(X_i)$破坏了交换$w\Leftrightarrow \widetilde{w}$与$X \Leftrightarrow X^T$时的对称性，但是这一项与$k$无关，所以我们通过将其融入关于$w_i$的bias项$b_i$来解决。</p>
<p>&emsp;&emsp;第二步为了平衡对称性，我们对应的再加入一个关于$\widetilde{w}_k$的bias项$\widetilde{b}_k$,然后我们可以得到$w_i^T\widetilde{w}_k+b_i+\widetilde{b}_k = \log(X_{ik})$</p>
<p>&emsp;&emsp;另一方面作者注意到模型的一个缺点是对于所有的co-occurrence的权重是一样的，所以他加入了前面的$f(X_{ij})$项来做weighted least squares regression模型，即为</p>
<script type="math/tex; mode=display">J = \sum_{i,j=1}^V {f(X_{ij})(w_i^T\widetilde{w}_j+b_i+\widetilde{b}_j-\log X_{ij})}^2</script><p>其中权重项$f$需满足以下条件</p>
<ul>
<li>$f(0)=0$,因为要求$\lim_{x\rightarrow 0} f(x)\log^2x$是有限的。</li>
<li>较少发生的co-occurrence占比重较小</li>
<li>对于较多发生的co-occurrence$f(x)$也不能过大</li>
</ul>
</blockquote>
<p>&emsp;&emsp;作者使用的较好的权重函数形式是：</p>
<script type="math/tex; mode=display">f(x)=\begin{cases}
(x/x_{max})^\alpha &\text{if $x\lt x_{max}$}\\
1 &\text{otherwise}
\end{cases}</script><p><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/lesson2_WeightedLeastSquaresRegression.PNG" alt></p>
<ul>
<li>模型优点<ul>
<li>训练快速</li>
<li>可以扩展到大型语料库</li>
<li>即使是小语料库和小向量，性能也很好</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>CS224N</category>
      </categories>
      <tags>
        <tag>CS224N</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224N笔记(一):word vector</title>
    <url>/2020/05/25/CS224N%E7%AC%94%E8%AE%B0-%E4%B8%80-word-vector/</url>
    <content><![CDATA[<p>&emsp;&emsp;受到导师的推荐来在线学习这门stanford课程CS224N，打下一些基础。这门课程的名称是<strong>Nature Language Processing with Deep Learning</strong>，即基于神经网络的自然语言处理,第一节上课的教授是<strong>Christopher Manning</strong>。老爷子精神很好，也很活泼。<br>&emsp;&emsp;这一节的主要内容是 </p>
<ul>
<li>nlp研究的对象</li>
<li>我们如何表示单词的含义（meaning of a word）</li>
<li>Word2Vec方法的基本原理</li>
</ul>
<p>&emsp;&emsp;自然语言，在这里指的当然就是人类语言，从整个地球史的角度来看，人类语言的出现距离我们当前年代是相当近的。教授认为，恰恰就是人类语言的出现区分了我们与猩猩（尽管它们也有自己的交流系统）。人类语言的奇妙之处在于它可以仅仅运用写在纸上的（莎草纸）的抽象文字来指代现实世界中的物理事物以及抽象概念，而想让机器做到与人一样理解语言是一项复杂的工作，当然这样也衍生出了许多有趣又实用的研究工作。<br>&emsp;&emsp;完成这些工作的前提是需要先解答这个问题：“我们如何表达一个单词的含义？”<br>&emsp;&emsp;一个早期常用的思想是我们将所有那些同义词与下义词建立成一个词库。课程内提到的<strong>wordNet</strong>便是运用这种方法的一个词库，在<strong>wordNet</strong>中一个词的含义由它的同义词集合与下义词集合来定义。显然，这样做会带来许多问题：</p>
<ul>
<li>有些词只在某些特定语境下被认为是同义词（例如：“proficient”只在一些情况下被认为是“good”的同义词</li>
<li>有时会丢失一些词语的新含义（例如该词被解构了）</li>
<li>定义的主观性</li>
<li>词库资料需要大量人力收集整理； </li>
<li>无法计算出准确的单词相似度</li>
</ul>
<p>&emsp;&emsp;那么向量呢，我们能否使用一个向量来表示出一个单词?一个简单的方法就是使用<strong>one-hot</strong>编码来表示单词，在一个向量中，只有代表该单词的位置元素被设置为1，而其他位置元素则都被设置为0，如下所示：<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/lesson1_one-hot.PNG" alt="one-hot"><br>&emsp;&emsp;向量的维数等于词库中的单词数目。显然，由于所有向量都是互相正交的，两个向量间的相似度无法被有效表示出来，并且向量维度过大。  </p>
<p>&emsp;</p>
<p>&emsp;&emsp;至此，我们还得寻找一种更好的表达单词含义的方式。这里借鉴了语言学家的思想： <strong><em>A word’s meaning is given by the words that frequently appear close-by”</em></strong>。含义类似于我们高中英语常做的阅读理解。我们定义一个由某个词为中心的定长的窗口，窗口内的其他词构成了它的上下文context，借助这些context我们可以建立对该词有效的表示。下图中banking附近常出现的词定义了banking的含义。<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/lesson1_banking.PNG" alt><br>&emsp;&emsp;前面提到的<strong>one-hot vector</strong>属于一种sparse vector,而我们想要构造的是dense vector即大多数元素不为零且维度较小的向量，并且希望在相似的上下文环境下word vector也较为相似。这里的word vector也可被称为word embedding 或是 word representation。<br>&emsp;&emsp;这里介绍了一种较为流行的方法<strong>Word2Vec</strong>来生成我们想要的word vector。它是由google的nlp专家在2013年的时候提出，其核心思想是已经我们有了很大的文本库（corpus of text,教授还介绍了一下拉丁语词根），当我们用固定窗口不断去扫过文本库中的句子时，我们有位于中间的center word c 以及其周边的单词们conetxt words o,而它们的相似度用$p(o|c)$表示，即给定c的情况下o的条件概率。我们不断的调整word vector使得该概率最大化。每个单词可用两个向量表示，一个是它作为center word时的向量$\mathcal{v}_w$与作为context word时的向量$\mathcal{u}_w$。<br>&emsp;<br>&emsp;&emsp;如下图所示是窗口大小为2，center word 为<em>into</em>的context word 的概率表示。<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/lesson1_centerword_into.PNG" alt><br>当我们扫到下一个位置时，banking就成为center word。<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/lesson1_centerword_banking.PNG" alt></p>
<p>对于每个位置t = 1,…T,在大小为m的固定窗口内预测上下文单词，给定中心词$w_t$  </p>
<script type="math/tex; mode=display">Likelihood = L(\theta) = {\prod_{t=1}^T}{\prod_{  {-m\leq j \leq m } \atop {j\neq 0}}}P(w_{t+j}|w_t;\theta)</script><ul>
<li>其中，$\theta$为所有需要优化的变量<br>目标函数$J(\theta)$(也可被称为代价函数或损失函数)是（平均）负对数似然。 <script type="math/tex; mode=display">J(\theta) = -{\frac{1}{T}}{\prod_{t=1}^T}{\prod_{  {-m\leq j \leq m } \atop {j\neq 0}}}logP(w_{t+j}|w_t;\theta)</script></li>
</ul>
<p>&emsp;&emsp;其中$log$形式是方便将连乘转化为求和，负号是希望将极大化似然率转化为极小化损失函数的等价问题。</p>
<blockquote>
<p>tips:在连乘之前使用log转化为求和非常常用与有效  </p>
<script type="math/tex; mode=display">log \prod_{i}{x_i} = \sum_ilog{x_i}</script></blockquote>
<p>&emsp;接下来的问题是，如何计算$P(w_{t+j}|w_t;\theta)$?<br>&emsp;答案是对于每个单词使用两个向量——</p>
<ul>
<li>$v_w$当$w$是中心词时</li>
<li>$u_w$当$w$是上下文词时</li>
<li>于是对于一个中心词$c$和一个上下文词$o$<script type="math/tex; mode=display">P(o|c) = \frac{\exp({u_o^T}v_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}</script><blockquote>
<p>公式中，向量$u_o$和向量$v_c$进行点乘，向量之间越相似，点乘结果越大，从而归一化后得到的概率值也越大。模型的训练是为了让具有相似上下文的单词，具有相似的向量。</p>
<h2 id="Word2vec-prediction-function"><a href="#Word2vec-prediction-function" class="headerlink" title="Word2vec prediction function"></a>Word2vec prediction function</h2><script type="math/tex; mode=display">P(o|c) = \frac{\exp(u_o^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}</script></blockquote>
</li>
<li>取幂使任何数都为正</li>
<li>点积的目的是比较$o$和$c$的相似性<script type="math/tex; mode=display">u^Tv = u.v = \sum_{i=1}^n{u_iv_i}</script></li>
<li>分母：对整个词汇表进行标准化，从而给出概率分布</li>
</ul>
<p><strong>softmax function</strong> &emsp;$\mathbb{R^n}\to\mathbb{R^n}$</p>
<script type="math/tex; mode=display">softmax(x_i) = \frac{\exp(x_i)}{\sum_{j=1}^n\exp(x_j)} = p_i</script><p>&emsp;&emsp;该函数可将任意值$x_i$映射到概率分布$p_i$</p>
<ul>
<li><strong>max</strong>:因为放大了最大的概率</li>
<li><strong>soft</strong>:因为仍然为较小的$x_i$赋予了一定概率</li>
<li>深度学习中常用<br>&emsp;&emsp;首先我们随机初始化$u_w\in\mathbb{R^d}$和$v_w\in\mathbb{R^d}$,而后使用梯度下降法进行更新。<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial}{\partial{v_c}}\log P(o|c) &= \frac{\partial}{\partial{v_c}}\log \frac{\exp({u_o^T}v_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}\\ 
&=\frac{\partial}{\partial{v_c}} \left( \log\exp(u_o^Tv_c))-\log\sum_{w\in V}\exp(u_w^Tv_c)\right)
\\
&=\frac{\partial}{\partial v_c} \left(u_o^Tv_c -\log\sum_{w\in V}\exp(u_w^Tv_c)\right)\\
&=u_o - \frac{\sum_{w\in V}\exp(u_w^Tv_c)u_w}{\sum_{w\in V}\exp(u_w^Tv_c)}
\end{aligned}</script></li>
</ul>
<p>&emsp;&emsp;可对上述结果重新排序如下，等式右边第一项是真正的上下文单词，第二项为预测的上下文单词，使用梯度下降法，模型的预测上下文将接近真正的上下文。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial}{\partial{v_c}}\log P(o|c) 
&= u_o - \frac{\sum_{w\in V}\exp(u_w^Tv_c)u_w}{\sum_{w\in V}\exp(u_w^Tv_c)}\\
&= u_o -\frac{\sum_{w\in V}\exp(u_w^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)} u_w\\
&=u_o - \sum_{w\in V} P(w|c)u_w
\end{aligned}</script><p>在$\sum_{w\in V} u_w^Tv_c$上对$u_o$取偏微分运算，注意这里$u_o$是$u_{w=0}$的缩写，故可知  </p>
<script type="math/tex; mode=display">\frac{\partial}{\partial u_o}\sum_{w\in V} u_w^Tv_c=\frac{\partial}{\partial u_o}u_o^Tv_c=v_c</script><p>&emsp;</p>
<p>因此：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial}{\partial{u_o}}\log P(o|c) 
&= \frac{\partial}{\partial{u_o}}\log \frac{\exp({u_o^T}v_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}\\
&= \frac{\partial}{\partial{u_o}} \left( \log\exp(u_o^Tv_c))-\log\sum_{w\in V}\exp(u_w^Tv_c)\right)
\\
&=\frac{\partial}{\partial u_o} \left(u_o^Tv_c -\log\sum_{w\in V}\exp(u_w^Tv_c)\right)\\
&=v_c - \frac{\sum_{w\in V}{\frac{\partial}{\partial u_o}\exp(u_w^Tv_c)}}{\sum_{w\in V}\exp(u_w^Tv_c)}\\
&=v_c - \frac{\exp(u_o^Tv_c)v_c}{\sum_{w\in V}\exp(u_w^Tv_c)}\\
&=v_c - \frac{\exp(u_o^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}v_c\\
&=v_c - P(o|c)v_c\\
&=(1-P(o|c))v_c  
\end{aligned}</script><p>&emsp;&emsp;当$P(o|c)\to1$时，即通过中心词$c$可以准确预测上下文词$o$，此时我们不需要调整$u_o$，反之，需要调整$u_o$。</p>
]]></content>
      <categories>
        <category>CS224N</category>
      </categories>
      <tags>
        <tag>nlp 机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224N笔记(五)-Dependency Parsing</title>
    <url>/2020/06/11/CS224N%E7%AC%94%E8%AE%B0-%E4%BA%94-Dependency-Parsing/</url>
    <content><![CDATA[<h1 id="Linguistic-Structure-Dependency-Parsing"><a href="#Linguistic-Structure-Dependency-Parsing" class="headerlink" title="Linguistic Structure:Dependency Parsing"></a>Linguistic Structure:Dependency Parsing</h1><p>&emsp;&emsp;对于<strong>句法结构（syntactic structure）</strong>而言,教授提到主要有两种方式：<strong>Constituency Parsing</strong>与<strong>Dependency Parsing</strong></p>
<h2 id="Constituency-Parsing"><a href="#Constituency-Parsing" class="headerlink" title="Constituency Parsing"></a>Constituency Parsing</h2><p>&emsp;&emsp;Constituency Parsing主要使用短语语法来不断地将词语整理成嵌套的组成成分，又被称为context-free grammers(CFG)</p>
<ul>
<li>短语结构将单词组织成嵌套的成分</li>
<li>单词被赋予了一个词性</li>
<li>短语可以递归形成更大的短语<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/CFGS.PNG" alt></li>
</ul>
<h2 id="Dependency-Parsing"><a href="#Dependency-Parsing" class="headerlink" title="Dependency Parsing"></a>Dependency Parsing</h2><p>&emsp;&emsp;不是使用各种类型的短语，而是直接通过单词与其他单词的关系表示句子的结构，显示哪些单词依赖于哪些其他单词<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/Dependemcy_Look.png" alt></p>
<p>&emsp;&emsp;为什么我们需要句子结构</p>
<ul>
<li>为了能够正确地解释语言，我们需要理解句子结构</li>
<li>人们通过将单词组合成更大的单元来传达复杂的思想，从而交流复杂的思想</li>
<li>我们需要知道哪些词是哪些词的修饰词，否则我们无法弄清句子是什么意思</li>
</ul>
<p>&emsp;&emsp;课堂上教授主要介绍了这几种类型的歧义：</p>
<ul>
<li>介词短语依附歧义：拿刀的是警察还是被杀的人？ <img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/介词短语依附歧义.png" alt><br>&emsp;  </li>
<li>协调范围模糊：主语是一个人还是两个人？<img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/协调范围模糊.png" alt><br>&emsp;  </li>
<li>形容词修饰语歧义：是直接（first hand）的工作还是第一份hand job？🤔<img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/handjob.png" alt><br>&emsp;  </li>
<li>动词短语依附歧义：文中的<strong><em>to be used for Olympic beach volleyball</em></strong> 作为动词短语修饰的是body还是beach?<img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/动词短语歧义.png" alt><br>&emsp;<br>&emsp;<br>&emsp;&emsp;面对复杂的句子结构，我们可能需要处理的数量级是指数级的，这个序列被称为<a href="https://zh.wikipedia.org/wiki/%E5%8D%A1%E5%A1%94%E5%85%B0%E6%95%B0" target="_blank" rel="noopener">卡特兰数</a><script type="math/tex; mode=display">C_{n} = \frac{(2n)!}{[(n+1)!n!]}</script>&emsp;&emsp;Dependency Structure展示了词语之间的依赖关系，通常用箭头表示其依存关系，有时也会在肩头上标出具体的语法关系。<br>&emsp;&emsp;Dependency Structure有两种表现形式，一种是直接在句子上标出依存关系箭头及语法关系：<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/箭头符号.PNG" alt><br>&emsp;&emsp;另一种是将其做成树状结构：<img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/树状图.PNG" alt><br>&emsp;&emsp;Dependency Parsing可以认为是给定输入句子$S=w_0w_1…w_n$(其中$w_0$经常是设置的fake root,使得每一个词都依赖于一个节点)构建对应的Dependency Tree Graph的任务，而<strong>Transition-based Dependency Parsing</strong>是其中的一个有效方法。<h3 id="Transition-based-Dependency-Parsing"><a href="#Transition-based-Dependency-Parsing" class="headerlink" title="Transition-based Dependency Parsing"></a>Transition-based Dependency Parsing</h3>&emsp;&emsp;可以将其看成一种<a href="https://zh.wikipedia.org/wiki/%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA" target="_blank" rel="noopener">状态机</a>,对于$S=w_0w_1…w_n$,state由三部分构成$(\sigma,\beta,A)$。<br>&emsp;&emsp;$\sigma$是$S$中若干$w_i$构成的stack<br>&emsp;&emsp;$\beta$是$S$中若干$w_i$构成的buffer<br>&emsp;&emsp;$A$是denpency arc(代表依赖关系的箭头)构成的集合，每一条边的形式是$(w_i,r,w_j)$,其中$r$描述了节点之间的依存关系。<br>&emsp;&emsp;初始状态下，$\sigma$仅包含root $w_0$,$\beta$包含了所有单词$w_1…w_n$,而A是空集合$\emptyset$。<img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/初始状态.PNG" alt></li>
</ul>
<p>&emsp;&emsp;最终目标是$\sigma$包含root $w_0$，$\beta$清空，而$A$包含了所有的dependency arc,则$A$就是我们想要的描述Denpendency的结果。<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/中止状态.PNG" alt><br>&emsp;&emsp;状态间的转换有三类</p>
<ul>
<li>SHIFT:将buffer中的第一个词移除并放到stack上</li>
<li>LEFT-ARC：将$(w_j,r,w_i)$加入边的集合$A$,其中$w_i$是stack上的次顶层的词，$w_j$是stack上最顶层的词。</li>
<li>RIGHT-ARC：将$(w_i,r,w_j)$加入边的集合$A$,其中$w_i$是stack上的次顶层的词，$w_j$是stack上最顶层的词。<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/流程.PNG" alt><br>&emsp;&emsp;我们不断的进行上述三类操作，直到从初始态达到最终态。在每个状态下如何选择哪种操作呢？当我们考虑到LEFT-ARC与RIGHT-ARC各有|R|（|R|为r的类的个数）种class，我们可以将其看做是class数为2|R|+1的分类问题，可以用SVM等传统机器学习方法解决。<h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3>&emsp;&emsp;针对上面的模型，我们有两种评估方式。一个是<strong>LAS（labeled attachment score）</strong>,只有在arc的箭头以及语法关系均正确时才算正确，另一种是<strong>UAS（unlabeled attachment score）</strong>,只要arc的箭头方向正确即可。<img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/Evaluation.PNG" alt><h3 id="Neural-Dependency-Parsing"><a href="#Neural-Dependency-Parsing" class="headerlink" title="Neural Dependency Parsing"></a>Neural Dependency Parsing</h3>&emsp;&emsp;传统的Transition-based Dependency Parsing对feature engineering要求较高，我们可以用神经网络来减少human labor。<br>&emsp;&emsp;对于Neural Dependency Parser，其输入特征通常包含三种：</li>
<li>stack和buffer中的单词及其dependent word。</li>
<li>单词的Part-of-Speech tag。</li>
<li>描述语法关系的arc label。<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/三类输入.jpg" alt><br>&emsp;&emsp;我们将其转换为embedding vector并将它们联结起来作为输入层，再经过若干非线性的隐藏层，最后加入softmax layer得到每个class的概率。<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/流程.jpg" alt></li>
</ul>
]]></content>
      <categories>
        <category>CS224N</category>
      </categories>
      <tags>
        <tag>nlp 机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>基于噪声不一致性的科学图像篡改检测——论文简述</title>
    <url>/2020/05/22/%E5%9F%BA%E4%BA%8E%E5%99%AA%E5%A3%B0%E4%B8%8D%E4%B8%80%E8%87%B4%E6%80%A7%E7%9A%84%E7%A7%91%E5%AD%A6%E5%9B%BE%E5%83%8F%E7%AF%A1%E6%94%B9%E6%A3%80%E6%B5%8B%E2%80%94%E2%80%94%E8%AE%BA%E6%96%87%E7%AE%80%E8%BF%B0/</url>
    <content><![CDATA[<h1 id="Scientific-Image-Tampering-Detection-Based-On-Noise-Inconsistencies-A-Method-And-Datasets"><a href="#Scientific-Image-Tampering-Detection-Based-On-Noise-Inconsistencies-A-Method-And-Datasets" class="headerlink" title="Scientific Image Tampering Detection Based On Noise Inconsistencies: A Method And Datasets"></a>Scientific Image Tampering Detection Based On Noise Inconsistencies: A Method And Datasets</h1><h2 id="主要背景"><a href="#主要背景" class="headerlink" title="主要背景"></a>主要背景</h2><p>&emsp;&emsp;由于数字图像使用的日渐频繁以及技术的日趋进步，科学图像造假的情况愈发严重。尽管可以用已有的检测技术直接运用在科学图像上，但是仍需要发展适配于该领域的算法，因为该领域有着独特的模式，格式与解决方式。同时，科技论文造假的数据库的建设也应当与算法的进步同步进行。<br>&emsp;&emsp;随着愈来愈多的造假图片进入了科学界，Bik在短时间内就有相当数量的发现，这也说明了通过评审去发现造假图片相当的困难。</p>
<h2 id="论文工作简要概述"><a href="#论文工作简要概述" class="headerlink" title="论文工作简要概述"></a>论文工作简要概述</h2><p>&emsp;&emsp;过去种种研究篡改图像的算法在面对不同的科学条件下常常会失效，本论文提出的基于噪声矛盾的算法可适宜于很多不同的科学领域。实验结果证明本方法可以健壮地检测各种场景下的图像操纵，并且表现超过了现存地同类型算法。该方法有望可以为检测嫌疑图像成为系统性步骤做出贡献。</p>
<h2 id="为何选择基于噪声矛盾算法"><a href="#为何选择基于噪声矛盾算法" class="headerlink" title="为何选择基于噪声矛盾算法"></a>为何选择基于噪声矛盾算法</h2><p>&emsp;&emsp;科学研究对图像处理已经相当宽容了，包括调整大小，对比度调整，锐化，白平衡都被认为是合理的预处理。而改变了上下文含义的行为则当然不被允许，如复制粘贴，切割移除等。对于专家而言，发现导致上下文语意不同的图像比较容易，而一些造假学者利用了人类很难去辨别噪声模式这一弱点。因此本文因此提出了一种基于发现噪声不一致性的科学图像篡改检测方法。</p>
<h2 id="算法特点"><a href="#算法特点" class="headerlink" title="算法特点"></a>算法特点</h2><ul>
<li>基于监督学习，可以从已有的数据集与新实例中学习</li>
<li>它适用于不同的分辨率和来自不同设备的图像</li>
<li>不局限于某一种图像格式</li>
<li>能够产生良好的预测以及一个小数据集</li>
<li>具有灵活性，可以针对不同科学场景进行参数调整</li>
</ul>
<h2 id="算法思路"><a href="#算法思路" class="headerlink" title="算法思路"></a>算法思路</h2><p>&emsp;&emsp;本算法运用了几种异构特征提取器，刚开始，一个输入图像会经过几种残差图像生成器，每种残差图像会有他们自己的特征提取器（可能会基于不同配置的特征提取方案），这种特征在后处理后会被送入分类器。<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/Figure_1.PNG" alt><br>&emsp;&emsp;特征提取方法大大减少了维数，减少了对训练数据量需求,可以在后端使用轻量级分类器，如逻辑回归或支持向量机。生成残差图像的方法数量与特征提取地参数增加了该方法的灵活性，并且不像神经网络参数调节那样地模糊，对于人类可以方便地手动调节。</p>
<h2 id="残差图片生成器"><a href="#残差图片生成器" class="headerlink" title="残差图片生成器"></a>残差图片生成器</h2><h3 id="Steganalytic-Filters"><a href="#Steganalytic-Filters" class="headerlink" title="Steganalytic Filters"></a>Steganalytic Filters</h3><p>&emsp;&emsp;Steganalytic Filters是从速记式加密技术处借鉴而来，将隐写术中嵌入数据地行为类推到图像操作。选择高通的滤波器，丢弃关于图像的内容，尽可能地强调噪音模式。</p>
<h3 id="Error-Level-Analysis-ELA"><a href="#Error-Level-Analysis-ELA" class="headerlink" title="Error Level Analysis (ELA)"></a>Error Level Analysis (ELA)</h3><p>&emsp;&emsp;该算法地思想是基于在JPEG压缩时引入的错误是非线性的，如果图像中的一部分具有与图像剩余部分不一致的JPEG质量因子，那么在压缩后可以与对应的默认质量因子平均的图片进行比较，比较其差异。</p>
<h3 id="Median-Filtering-Residual"><a href="#Median-Filtering-Residual" class="headerlink" title="Median Filtering Residual"></a>Median Filtering Residual</h3><p>&emsp;&emsp;中值滤波可以抑制图像噪声，篡改的部分中值滤波后会表现出不同的噪声模式。残差图像是原始图像与中值滤波后的图像的差。</p>
<h3 id="Wavelet-Denoising-Residual"><a href="#Wavelet-Denoising-Residual" class="headerlink" title="Wavelet Denoising Residual"></a>Wavelet Denoising Residual</h3><p>&emsp;&emsp;小波去噪是一种在小波域内对图像进行去噪的方法，它基于小波域去噪来消除图像中的噪声。该算法运用方法与前一种中值滤波法类似。</p>
<p>&emsp;</p>
<p>&emsp;&emsp;示例中的残差图像算法在论文的例子中都有效果，但这并不是常态，真实情况往往需要几种算法结合起来分析。</p>
<h2 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h2><p>&emsp;&emsp;在该论文中特征提取是基于patch而不是基于像素点，这减少了计算规模并丰富了每个最小单元内的统计信息量，patch size决定后会生成对应的特征向量v。</p>
<h3 id="Patch-Reinterpretation"><a href="#Patch-Reinterpretation" class="headerlink" title="Patch Reinterpretation"></a>Patch Reinterpretation</h3><p>&emsp;&emsp;残差降低了图像数据的复杂性，但残差图像仍然具有与原始图像相同的维数。<br>&emsp;&emsp;为了进一步压缩数据进行分类，我们提出了一种新的图像篡改检测的特征提取方法。直观地说，一个图像区域被认为是被篡改的，不是因为它本身是唯一的，而是因为它与图像的其他部分不同。因此，一个理想的特征设计应该包含足够的全局信息。我们通过使用图像的其余部分重新解释图像区域来添加全局信息。<br>&emsp;&emsp;首先对于大小为（h;w）的图像会被分割为大小为（m;n）的图像，如果不可正好分割，就向下取最近整数，由此大小为（h;w）的图像就被成为了一个patch矩阵。然后将patch矩阵分割成大小为(s;t)，其中每个单元包含一定数量的patch。<br><img src="https://bwtpicturehouse.oss-cn-shanghai.aliyuncs.com/img/Figure_2.PNG" alt></p>
<p>&emsp;&emsp;对于网格中的每个细胞，我们都安装了一个异常值检测器，它能够告诉新样本成为异常值的可能性。给定一个p，它可以被向量v重新解释,  </p>
<script type="math/tex; mode=display">v = (l_{11}(p), l_{12}(p),l_{13}(p),...,l_{1t}(p)\\
    \qquad l_{21}(p), l_{22}(p),l_{23}(p),...,l_{2t}(p) \\
     \qquad l_{31}(p), l_{32}(p),l_{33}(p),...,l_{1t}(p)\\
     \qquad...\\
     \qquad l_{s1}(p), l_{s2}(p),l_{s3}(p),... ,l_{st}(p)).</script><p>&emsp;&emsp;由于被篡改区域的残馀形态不同，且其污染斑块集中在其中一个cell内，<br>因此该cell的离群点检测器与其他cell相比具有不同的判定边界。<br>&emsp;&emsp;Eg.对于真实的patch pa,除了l23(pa)外别的离散概率都会较低；而对于伪造的patch pt,除了l23(pt)外别的离散概率都会较高。<br>&emsp;&emsp;在实际应用中，我们使用了v的直方图(用vh表示)，它不仅在摘要统计，空间中对结构进行编码，而且保留了位置信息。  </p>
<p>&emsp;</p>
<h3 id="Feature-Design"><a href="#Feature-Design" class="headerlink" title="Feature Design"></a>Feature Design</h3><ul>
<li>vh:Patch Reinterpretation的直方图</li>
<li>距离信息:patch与它邻居的欧式距离</li>
<li>全局信息:patch与全图的信息差异，patch的附加全局信息是由重新解释和聚类中心之间的欧氏距离以及中心的相应权重给出的。  </li>
</ul>
<p>&emsp;</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="生成数据集"><a href="#生成数据集" class="headerlink" title="生成数据集"></a>生成数据集</h3><h4 id="移除类型"><a href="#移除类型" class="headerlink" title="移除类型"></a>移除类型</h4><p>&emsp;&emsp;删除一个区域，并选用某一背景区域的噪声或色彩去采样填补这一区域。</p>
<h4 id="拼接类型"><a href="#拼接类型" class="headerlink" title="拼接类型"></a>拼接类型</h4><p>&emsp;&emsp;从前景图像中选取一小部分黏贴到任意位置，该区域要么用JPEG重新压缩，要么用锐化过滤器处理。</p>
<h4 id="修整类型"><a href="#修整类型" class="headerlink" title="修整类型"></a>修整类型</h4><p>&emsp;&emsp;随机对一块区域使用高斯模糊。  </p>
<p>&emsp;</p>
<p>最终生成的图像有蛋白质印记法与显微镜观测两种类型</p>
<h3 id="实验配置"><a href="#实验配置" class="headerlink" title="实验配置"></a>实验配置</h3><ul>
<li>利用离散余弦变换(DCT)将残差图像上的小块变换到频域，得到了较好的效果</li>
<li>使用由scikit-learn提供的支持向量机离群点检测器，其核函数使用径向基函数</li>
<li>优化公差设置为0:01;(训练误差分数的上界和支持向量分数的下界)设为0.1</li>
<li>不同参数的设置会较大影响特征提取的速度。</li>
<li>使用的分类器是一个简单的多层结构感知器神经网络。对于蛋白质标记模型，我们使用四层网络，每层200个unit;对于显微模型，我们使用类似的网络，每层300个unit。最后一层应用Softmax回归得到分类结果。</li>
</ul>
<p>&emsp;</p>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>&emsp;&emsp;在使用的是patch级别的准确性，AUC分数和F1分数的评判下,总的来说，该论文方法在不同类型的操作之间的性能更一致，这使它在实践中更可靠。</p>
<p>&emsp;</p>
<h2 id="研究中的不足"><a href="#研究中的不足" class="headerlink" title="研究中的不足"></a>研究中的不足</h2><ul>
<li>数据集还是过小，但相信随着研究深入会逐渐扩充</li>
<li>只基于噪声的方法会受到限制，例如一些图像篡改不会影响到噪声。如果一个人已经了解了自动检测的原理，他可以有意避开噪声不一致性</li>
</ul>
<p>&emsp;&emsp;最终作者希望这种算法的灵活性可以帮助其顺利迁移到更多其他领域。</p>
]]></content>
      <categories>
        <category>篡改检测</category>
      </categories>
      <tags>
        <tag>科学图像篡改检测</tag>
        <tag>论文简述</tag>
      </tags>
  </entry>
  <entry>
    <title>hexo 常用命令小记</title>
    <url>/2020/06/02/hexo-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%B0%8F%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="hexo常用命令小记"><a href="#hexo常用命令小记" class="headerlink" title="hexo常用命令小记"></a>hexo常用命令小记</h1><p>&emsp;&emsp;最近受到社会的毒打，我决定搭建一个属于自己的技术博客，好让自己对学到的知识做到温故知新，不要再出现复试里的尴尬场面了😳所以接下来这篇是我的处女作(惭愧)，介绍一下常用的hexo命令作为小抄。</p>
<p>&emsp;</p>
<h2 id="新建一篇博客文章"><a href="#新建一篇博客文章" class="headerlink" title="新建一篇博客文章"></a>新建一篇博客文章</h2><p>&emsp;&emsp;进入你的博客目录，在路径/source/_posts文件夹下右键选择Git Bash Here进入命令窗口，利用<code>hexo new</code>命令来创建新文章。<br><code>hexo new &quot;hexo常用命令小记&quot; #新建文章</code><br>此时，你会发现在/source/_posts文件夹里有了 hexoc常用命令小记.md 这个文件,用markdown语法编辑它吧！</p>
<p>&emsp;</p>
<h2 id="给文章添加分类与标签"><a href="#给文章添加分类与标签" class="headerlink" title="给文章添加分类与标签"></a>给文章添加分类与标签</h2><p>&emsp;&emsp;在 常用命令小记.md中可设置tags和categories属性<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">title: hexo 常用命令小记</span><br><span class="line">tags:</span><br><span class="line">- 博客</span><br><span class="line">- hexo</span><br><span class="line">categories: web前端</span><br></pre></td></tr></table></figure><br>&emsp;</p>
<h2 id="启动本地服务器，本地测试"><a href="#启动本地服务器，本地测试" class="headerlink" title="启动本地服务器，本地测试"></a>启动本地服务器，本地测试</h2><p><code>hexo s</code><br>在本地<code>http://localhost:4000</code>上可以预览文章<br>如图所示：<br><img src="/.io//catch.jpg" alt></p>
<p>&emsp; </p>
<h2 id="正式发布新建的博客文章"><a href="#正式发布新建的博客文章" class="headerlink" title="正式发布新建的博客文章"></a>正式发布新建的博客文章</h2><p>&emsp;&emsp;在博客项目文件夹下再次运行Git Bash Here命令窗口，输入以下代码:<br><code>hexo generate # 生成更改</code><br><code>hexo deploy   # 将生成的更改部署到github上</code><br>可以在gitpage上看到自己的这篇博客啦！</p>
]]></content>
      <categories>
        <category>web前端</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
